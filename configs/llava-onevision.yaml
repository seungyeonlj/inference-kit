model_type: llava-onevision
disable_mm_preprocessor_cache: False  # If True, disables caching of multi-modal preprocessor/mapper.
# max_seq_len: 16384

dtype: half
tensor_parallel_size: 1

temperature: 0.0  # Temperature for sampling.

datasets:
  - dataset_path: "./data/all_data.json"
    dataset_name: "multimodal_rewardbench"
    dataset_split: test
    load_local_image: True
    image_folder: "./data/"
    swap: False
  - dataset_path: "MMInstruction/VL-RewardBench"
    dataset_name: "vl_rewardbench"
    dataset_split: test
    load_local_image: False    
    swap: False

num_prompts: 10
modality: image

seed: 123  # Set the seed when initializing `vllm.LLM`.

time_generate: True

data_chunk_size: 100
start_idx: 0

output_dir: results/
