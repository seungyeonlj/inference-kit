model_type: llava-onevision
disable_mm_preprocessor_cache: False  # If True, disables caching of multi-modal preprocessor/mapper.
# max_seq_len: 16384

dtype: half
tensor_parallel_size: 1

temperature: 0.0  # Temperature for sampling.

dataset: "/workspace/language_model/data/all_data.json"
dataset_split: test
load_local_image: True
image_folder: "/workspace/multimodal_rewardbench/data/"
swap: False

num_prompts: 10
modality: image

seed: 123  # Set the seed when initializing `vllm.LLM`.

time_generate: True

data_chunk_size: 100
start_idx: 0

output_dir: results/